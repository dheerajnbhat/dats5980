{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3ae3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7da7426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\InstalledApps\\miniconda3\\envs\\my\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Dict, List, Tuple, Set, Iterable, Optional\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4467a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34649597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate embedder (will download model if necessary)\n",
    "embedder = Embedder(model_name=\"all-MiniLM-L6-v2\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40eeb462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bring', 'umbrella', 'sunny', 'day']\n"
     ]
    }
   ],
   "source": [
    "def extract_keywords_spacy(question):\n",
    "    doc = nlp(question)\n",
    "\n",
    "    keywords = []\n",
    "    for token in doc:\n",
    "        # Filter out stopwords, punctuation, and select meaningful POS\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            if token.pos_ in {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\"}:\n",
    "                keywords.append(token.lemma_.lower())\n",
    "\n",
    "    return list(dict.fromkeys(keywords))  # Remove duplicates, preserve order\n",
    "\n",
    "\n",
    "# Example\n",
    "question = \"Why would someone bring an umbrella outside on a sunny day?\"\n",
    "print(extract_keywords_spacy(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b808db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5230"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"data/conceptnet_relations.json\", \"r\") as fp:\n",
    "    conceptnet_relations = json.load(fp)\n",
    "\n",
    "len(conceptnet_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e977673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "kg_to_context.py\n",
    "\n",
    "End-to-end pipeline:\n",
    "  Knowledge Graph -> Text Triples -> Embeddings -> Ranking -> Context\n",
    "\n",
    "Requirements:\n",
    "  pip install networkx sentence-transformers numpy tqdm\n",
    "\n",
    "By default this uses sentence-transformers \"all-MiniLM-L6-v2\".\n",
    "You may swap the embedder to any model that returns dense vectors.\n",
    "\n",
    "Usage:\n",
    "  - Put your ConceptNet-like data in a dict: {entity: {relation: [(target, score), ...], ...}, ...}\n",
    "  - Call build_graph_from_conceptnet_dicts(...)\n",
    "  - Call create_context_for_question(...)\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------\n",
    "# Types\n",
    "# -------------------------\n",
    "CNNode = str\n",
    "Relation = str\n",
    "Score = float\n",
    "CNData = Dict[Relation, List[Tuple[CNNode, Score]]]  # like the dict you provided\n",
    "Triple = Tuple[str, str, str]  # (subject, relation, object)\n",
    "\n",
    "# -------------------------\n",
    "# Graph construction\n",
    "# -------------------------\n",
    "def build_graph_from_conceptnet_dicts(conceptnet_dict_by_word: Dict[str, CNData], add_reverse_edges: bool=True\n",
    "                                     ) -> nx.MultiDiGraph:\n",
    "    \"\"\"\n",
    "    Build a MultiDiGraph where nodes are concept strings and edges have attribute 'rel'.\n",
    "    Uses ALL relations found (scores are ignored but preserved if you want later).\n",
    "    Optionally adds reverse edges labeled 'rev:<REL>' for easier path finding.\n",
    "    \"\"\"\n",
    "    G = nx.MultiDiGraph()\n",
    "    for subj, rels in conceptnet_dict_by_word.items():\n",
    "        G.add_node(subj)\n",
    "        for rel, obj_list in rels.items():\n",
    "            for obj, score in obj_list:\n",
    "                G.add_node(obj)\n",
    "                G.add_edge(subj, obj, rel=rel, score=score)\n",
    "                if add_reverse_edges:\n",
    "                    G.add_edge(obj, subj, rel=f\"rev:{rel}\", score=score)\n",
    "    return G\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Triple extraction utilities\n",
    "# -------------------------\n",
    "def triples_from_edge(u: str, v: str, attrs: Dict) -> List[Triple]:\n",
    "    \"\"\"\n",
    "    Create triple(s) from edge attrs in MultiDiGraph.\n",
    "    For multi-edges we'll get multiple triples (same subj,obj but different rel).\n",
    "    \"\"\"\n",
    "    rel = attrs.get(\"rel\", \"RelatedTo\")\n",
    "    return [(u, rel, v)]\n",
    "\n",
    "def triples_from_path(G: nx.MultiDiGraph, path: List[str]) -> List[Triple]:\n",
    "    \"\"\"\n",
    "    Given a node path [n0, n1, n2, ...], return the list of triple facts for each adjacent pair.\n",
    "    For each adjacent pair, we return a triple for every multiedge's 'rel'.\n",
    "    \"\"\"\n",
    "    triples: List[Triple] = []\n",
    "    for i in range(len(path) - 1):\n",
    "        u, v = path[i], path[i + 1]\n",
    "        data = G.get_edge_data(u, v, default={})\n",
    "        # get_edge_data returns dict keyed by keys for MultiDiGraph; each entry is attr dict\n",
    "        if data:\n",
    "            for k, attrs in data.items():\n",
    "                rel = attrs.get(\"rel\", \"RelatedTo\")\n",
    "                triples.append((u, rel, v))\n",
    "        else:\n",
    "            # fallback - shouldn't happen\n",
    "            triples.append((u, \"RelatedTo\", v))\n",
    "    return triples\n",
    "\n",
    "\n",
    "def extract_candidate_triples(\n",
    "    G: nx.MultiDiGraph,\n",
    "    question_concept: str,\n",
    "    keywords: Iterable[str],\n",
    "    choices: Iterable[str],\n",
    "    max_hops: int = 2,\n",
    "    include_1hop_from_choices: bool = True,\n",
    "    include_1hop_from_question_concept: bool = True,\n",
    "    top_k_paths_per_pair: int = 40\n",
    ") -> Dict[str, Set[Triple]]:\n",
    "    \"\"\"\n",
    "    For each choice, collect candidate triples connecting:\n",
    "      - question_concept -> choice (paths up to max_hops)\n",
    "      - keywords -> choice (paths up to max_hops)\n",
    "      - optionally 1-hop neighbors of choice (triples touching the choice)\n",
    "      - optionally 1-hop neighbors of question_concept\n",
    "\n",
    "    Returns: { choice: set_of_triples }\n",
    "    NOTE: This uses structure only to collect candidates; ranking will be embedding-based.\n",
    "    \"\"\"\n",
    "    choices = list(choices)\n",
    "    keywords = list(keywords)\n",
    "    facts_by_choice: Dict[str, Set[Triple]] = {c: set() for c in choices}\n",
    "\n",
    "    nodes_present = set(G.nodes())\n",
    "\n",
    "    def get_paths(src, tgt, cutoff):\n",
    "        if src not in nodes_present or tgt not in nodes_present:\n",
    "            return []\n",
    "        try:\n",
    "            # all_simple_paths considers number of edges <= cutoff\n",
    "            paths = list(nx.all_simple_paths(G, source=src, target=tgt, cutoff=cutoff))\n",
    "        except nx.NetworkXNoPath:\n",
    "            return []\n",
    "        paths.sort(key=lambda p: len(p))\n",
    "        return paths[:top_k_paths_per_pair]\n",
    "\n",
    "    # 1) collect paths from question_concept -> choice\n",
    "    if include_1hop_from_question_concept:\n",
    "        for c in choices:\n",
    "            paths = get_paths(question_concept, c, cutoff=max_hops)\n",
    "            for p in paths:\n",
    "                for t in triples_from_path(G, p):\n",
    "                    facts_by_choice[c].add(t)\n",
    "\n",
    "    # 2) collect paths from each keyword -> choice\n",
    "    for kw in keywords:\n",
    "        for c in choices:\n",
    "            paths = get_paths(kw, c, cutoff=max_hops)\n",
    "            for p in paths:\n",
    "                for t in triples_from_path(G, p):\n",
    "                    facts_by_choice[c].add(t)\n",
    "\n",
    "    # 3) include direct edges touching the choice (1-hop outgoing & incoming)\n",
    "    if include_1hop_from_choices:\n",
    "        for c in choices:\n",
    "            if c not in nodes_present:\n",
    "                continue\n",
    "            # outgoing edges c -> neighbor\n",
    "            for _, neigh, data in G.out_edges(c, data=True):\n",
    "                triples = triples_from_path(G, [c, neigh])\n",
    "                for t in triples:\n",
    "                    facts_by_choice[c].add(t)\n",
    "            # incoming edges neigh -> c\n",
    "            for neigh, _, data in G.in_edges(c, data=True):\n",
    "                triples = triples_from_path(G, [neigh, c])\n",
    "                for t in triples:\n",
    "                    facts_by_choice[c].add(t)\n",
    "\n",
    "    return facts_by_choice\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Text conversion for triples\n",
    "# -------------------------\n",
    "def triple_to_text(triple: Triple) -> str:\n",
    "    \"\"\"\n",
    "    Convert triple to a short atomic fact string that is stable and compact.\n",
    "    Example: (\"scissors\", \"UsedFor\", \"cut\") -> \"scissors UsedFor cut\"\n",
    "    \"\"\"\n",
    "    s, rel, o = triple\n",
    "    # normalize spacing\n",
    "    s = s.strip()\n",
    "    rel = rel.strip()\n",
    "    o = o.strip()\n",
    "    return f\"{s} {rel} {o}\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Embedding utilities\n",
    "# -------------------------\n",
    "class Embedder:\n",
    "    \"\"\"\n",
    "    Simple wrapper around sentence-transformers / any embedding model with encode() method.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", device: str = \"cpu\"):\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def embed_texts(self, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return L2-normalized numpy array of embeddings shape (N, D)\n",
    "        \"\"\"\n",
    "        embs = self.model.encode(texts, batch_size=batch_size, show_progress_bar=False, convert_to_numpy=True)\n",
    "        # normalize\n",
    "        norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "        norms[norms == 0.0] = 1.0\n",
    "        embs = embs / norms\n",
    "        return embs\n",
    "\n",
    "\n",
    "def cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity matrix between two normalized embedding sets A and B.\n",
    "    Assumes rows are normalized.\n",
    "    Returns matrix shape (A_rows, B_rows)\n",
    "    \"\"\"\n",
    "    return np.dot(A, B.T)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Ranking & context builder\n",
    "# -------------------------\n",
    "def rank_triples_for_choices(\n",
    "    question: str,\n",
    "    keywords: Iterable[str],\n",
    "    facts_by_choice: Dict[str, Set[Triple]],\n",
    "    embedder: Embedder,\n",
    "    top_k_per_choice: int = 8,\n",
    "    combine_with_keywords: bool = True,\n",
    "    weight_question: float = 0.6,\n",
    "    weight_keywords: float = 0.4\n",
    ") -> Dict[str, List[Tuple[Triple, float]]]:\n",
    "    \"\"\"\n",
    "    Rank triples per choice by semantic similarity to the question (and optionally keywords).\n",
    "    Returns: { choice: [ (triple, score), ... ] } sorted descending score, up to top_k_per_choice.\n",
    "    Notes:\n",
    "      - We compute embeddings for all triples once (deduplicated across choices).\n",
    "      - Scoring: score = weight_question * sim(triple, question) + weight_keywords * max_sim(triple, any_keyword)\n",
    "    \"\"\"\n",
    "    # prepare texts\n",
    "    all_triples = set()\n",
    "    for c, facts in facts_by_choice.items():\n",
    "        all_triples.update(facts)\n",
    "    all_triples = sorted(all_triples)  # deterministic order\n",
    "\n",
    "    triple_texts = [triple_to_text(t) for t in all_triples]\n",
    "    # embed triples and question+keywords\n",
    "    triple_embs = embedder.embed_texts(triple_texts)\n",
    "    q_emb = embedder.embed_texts([question])[0:1]  # shape (1,D)\n",
    "    if combine_with_keywords:\n",
    "        kw_texts = list(keywords)\n",
    "        if len(kw_texts) == 0:\n",
    "            kw_embs = None\n",
    "        else:\n",
    "            kw_embs = embedder.embed_texts(kw_texts)\n",
    "    else:\n",
    "        kw_embs = None\n",
    "\n",
    "    # compute sims\n",
    "    sim_q = cosine_sim_matrix(triple_embs, q_emb).squeeze(axis=1)  # shape (num_triples,)\n",
    "    if kw_embs is not None:\n",
    "        sim_kw = cosine_sim_matrix(triple_embs, kw_embs)  # (num_triples, num_kw)\n",
    "        sim_kw_max = sim_kw.max(axis=1)\n",
    "    else:\n",
    "        sim_kw_max = np.zeros_like(sim_q)\n",
    "\n",
    "    # final score\n",
    "    scores = weight_question * sim_q + (weight_keywords * sim_kw_max if kw_embs is not None else 0.0)\n",
    "\n",
    "    # map back to choices\n",
    "    triple_to_score = {all_triples[i]: float(scores[i]) for i in range(len(all_triples))}\n",
    "\n",
    "    ranked_by_choice: Dict[str, List[Tuple[Triple, float]]] = {}\n",
    "    for c, facts in facts_by_choice.items():\n",
    "        scored = [(t, triple_to_score.get(t, -1.0)) for t in facts]\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        ranked_by_choice[c] = scored[:top_k_per_choice]\n",
    "    return ranked_by_choice\n",
    "\n",
    "\n",
    "def build_choice_grouped_context(\n",
    "    question: str,\n",
    "    choices: List[str],\n",
    "    ranked_triples_by_choice: Dict[str, List[Tuple[Triple, float]]],\n",
    "    facts_per_choice_limit: int = 6\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a compact prompt context grouped by choice.\n",
    "    We include only top facts_per_choice_limit per choice.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"Use the facts below (extracted from ConceptNet) to answer the question.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Question:\")\n",
    "    lines.append(question)\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Choices:\")\n",
    "    for idx, c in enumerate(choices):\n",
    "        lines.append(f\"{chr(ord('A') + idx)}. {c}\")\n",
    "    lines.append(\"\\nRelevant facts (grouped by choice):\")\n",
    "\n",
    "    for idx, c in enumerate(choices):\n",
    "        lines.append(f\"\\nChoice {chr(ord('A') + idx)}: {c}\")\n",
    "        facts = ranked_triples_by_choice.get(c, [])\n",
    "        if not facts:\n",
    "            lines.append(\"  - (no facts found)\")\n",
    "            continue\n",
    "        for (t, score) in facts[:facts_per_choice_limit]:\n",
    "            lines.append(f\"  - {triple_to_text(t)}  [score={score:.3f}]\")\n",
    "    lines.append(\"\\nTask: Based only on the facts above, choose the best answer and briefly explain using those facts.\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def build_choice_grouped_context_modified_2(\n",
    "    question: str,\n",
    "    choices: List[str],\n",
    "    ranked_triples_by_choice: Dict[str, List[Tuple[Triple, float]]],\n",
    "    facts_per_choice_limit: int = 5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a compact prompt context grouped by choice.\n",
    "    We include only top facts_per_choice_limit per choice.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    # lines.append(\"Use the facts below (extracted from ConceptNet) to answer the question.\")\n",
    "    # lines.append(\"\")\n",
    "    # lines.append(\"Question:\")\n",
    "    # lines.append(question)\n",
    "    # lines.append(\"\")\n",
    "    # lines.append(\"Choices:\")\n",
    "    # for idx, c in enumerate(choices):\n",
    "    #     lines.append(f\"{chr(ord('A') + idx)}. {c}\")\n",
    "    # lines.append(\"\\nRelevant facts (grouped by choice):\")\n",
    "\n",
    "    for idx, c in enumerate(choices):\n",
    "        lines.append(f\"\\nChoice {chr(ord('A') + idx)}: {c}\")\n",
    "        facts = ranked_triples_by_choice.get(c, [])\n",
    "        if not facts:\n",
    "            lines.append(\"  - (no facts found)\")\n",
    "            continue\n",
    "        for (t, score) in facts[:facts_per_choice_limit]:\n",
    "            # lines.append(f\"  - {triple_to_text(t)}  [score={score:.3f}]\")\n",
    "            lines.append(f\"  - {triple_to_text(t)}\")\n",
    "    # lines.append(\"\\nTask: Based only on the facts above, choose the best answer and briefly explain using those facts.\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_choice_grouped_context_modified(\n",
    "    question: str,\n",
    "    choices: List[str],\n",
    "    ranked_triples_by_choice: Dict[str, List[Tuple[Triple, float]]],\n",
    "    facts_per_choice_limit: int = 6,\n",
    "    top_k_total_facts: int = 5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a compact prompt context grouped by choice.\n",
    "    We include only top facts_per_choice_limit per choice.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    # lines.append(\"Use the facts below (extracted from ConceptNet) to answer the question.\")\n",
    "    # lines.append(\"\")\n",
    "    # lines.append(\"Question:\")\n",
    "    # lines.append(question)\n",
    "    # lines.append(\"\")\n",
    "    # lines.append(\"Choices:\")\n",
    "    # for idx, c in enumerate(choices):\n",
    "    #     lines.append(f\"{chr(ord('A') + idx)}. {c}\")\n",
    "    # lines.append(\"\\nRelevant facts:\")\n",
    "\n",
    "    total_ranked_facts = []\n",
    "    for idx, c in enumerate(choices):\n",
    "        # lines.append(f\"\\nChoice {chr(ord('A') + idx)}: {c}\")\n",
    "        facts = ranked_triples_by_choice.get(c, [])\n",
    "        if not facts:\n",
    "            # lines.append(\"  - (no facts found)\")\n",
    "            continue\n",
    "        for (t, score) in facts[:facts_per_choice_limit]:\n",
    "            # total_ranked_facts.append((c, t, triple_to_text(t), score))\n",
    "            total_ranked_facts.append((c, t, triple_to_text(t), score))\n",
    "        # for (t, score) in facts[:facts_per_choice_limit]:\n",
    "        #     lines.append(f\"  - {triple_to_text(t)}  [score={score:.3f}]\")\n",
    "    \n",
    "    # print(\"=== Total Ranked Facts Across Choices ===\")\n",
    "    total_ranked_facts.sort(key=lambda x: x[3], reverse=True)\n",
    "    # print(total_ranked_facts)\n",
    "\n",
    "    for item in total_ranked_facts[: top_k_total_facts]:\n",
    "        c, t, text, score = item\n",
    "        lines.append(f\"{text} [score={score:.3f}]\")\n",
    "        # print(lines)\n",
    "    \n",
    "    # lines.append(\"\\nTask: Based only on the facts above, choose the best answer and briefly explain using those facts.\")\n",
    "    # return \"\\n\".join(lines)\n",
    "    return lines\n",
    "\n",
    "# -------------------------\n",
    "# End-to-end wrapper\n",
    "# -------------------------\n",
    "def create_context_for_question(\n",
    "    G: nx.MultiDiGraph,\n",
    "    conceptnet_map: Dict[str, CNData],\n",
    "    question: str,\n",
    "    question_concept: str,\n",
    "    keywords: Iterable[str],\n",
    "    choices: Iterable[str],\n",
    "    embedder: Optional[Embedder] = None,\n",
    "    max_hops: int = 2,\n",
    "    top_k_paths_per_pair: int = 40,\n",
    "    top_k_triples_per_choice: int = 10,\n",
    "    facts_per_choice_limit: int = 6\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    End-to-end: extract candidate triples from KG -> embed/rank -> build grouped context prompt.\n",
    "    Returns the prompt text to feed to smolVLM.\n",
    "    \"\"\"\n",
    "    if embedder is None:\n",
    "        embedder = Embedder()\n",
    "\n",
    "    # 1) build facts_by_choice using graph (structure only)\n",
    "    facts_by_choice = extract_candidate_triples(\n",
    "        G,\n",
    "        question_concept,\n",
    "        keywords,\n",
    "        choices,\n",
    "        max_hops=max_hops,\n",
    "        include_1hop_from_choices=True,\n",
    "        include_1hop_from_question_concept=True,\n",
    "        top_k_paths_per_pair=top_k_paths_per_pair\n",
    "    )\n",
    "\n",
    "    # 2) rank triples using embeddings\n",
    "    ranked = rank_triples_for_choices(\n",
    "        question=question,\n",
    "        keywords=keywords,\n",
    "        facts_by_choice=facts_by_choice,\n",
    "        embedder=embedder,\n",
    "        top_k_per_choice=top_k_triples_per_choice,\n",
    "        combine_with_keywords=True\n",
    "    )\n",
    "\n",
    "    # 3) build grouped context\n",
    "    prompt = build_choice_grouped_context_modified_2(\n",
    "        question=question,\n",
    "        choices=list(choices),\n",
    "        ranked_triples_by_choice=ranked,\n",
    "        facts_per_choice_limit=facts_per_choice_limit\n",
    "    )\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fe77062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choice A: bank\n",
      "  - revolving door AtLocation bank\n",
      "  - bank AtLocation secure place\n",
      "  - bank UsedFor keeping money safe\n",
      "  - bank RelatedTo vault\n",
      "  - bank RelatedTo safe\n",
      "  - bank RelatedTo robbery\n",
      "\n",
      "Choice B: library\n",
      "  - library AtLocation house\n",
      "  - library AtLocation computers\n",
      "  - library DerivedFrom cyberlibrary\n",
      "  - library AtLocation human\n",
      "  - library AtLocation literature\n",
      "  - library Antonym book\n",
      "\n",
      "Choice C: department store\n",
      "  - revolving door AtLocation department store\n",
      "  - department store AtLocation escalator\n",
      "  - department store AtLocation changing room\n",
      "  - department store UsedFor anchor mall\n",
      "  - department store RelatedTo gum\n",
      "  - department store AtLocation fitting room\n",
      "\n",
      "Choice D: mall\n",
      "  - revolving door AtLocation department store\n",
      "  - revolving door AtLocation bank\n",
      "  - revolving door AtLocation mall\n",
      "  - mall AtLocation escalator\n",
      "  - mall RelatedTo mall walker\n",
      "  - mall UsedFor concentrated foot traffic\n",
      "\n",
      "Choice E: new york\n",
      "  - revolving door AtLocation department store\n",
      "  - new york RelatedTo liberty\n",
      "  - new york AtLocation mouse\n",
      "  - new york AtLocation kosher restaurant\n",
      "  - new york RelatedTo wall street\n",
      "  - new york AtLocation subway station\n"
     ]
    }
   ],
   "source": [
    "question = \"A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\"\n",
    "question_concept = \"revolving door\"\n",
    "choices = [ \"bank\", \"library\", \"department store\", \"mall\", \"new york\" ]\n",
    "\n",
    "# question = \"What do people aim to do at work?\"\n",
    "# question_concept = \"people\"\n",
    "# choices = ['complete job', 'learn from each other', 'kill animals', 'wear hats', 'talk to each other']\n",
    "\n",
    "conceptnet_map = {\n",
    "    c: conceptnet_relations[c] for c in [question_concept] + choices\n",
    "}\n",
    "keywords = extract_keywords_spacy(question)          # extracted keywords\n",
    "\n",
    "# Build graph\n",
    "G = build_graph_from_conceptnet_dicts(conceptnet_map, add_reverse_edges=False)\n",
    "\n",
    "# create prompt/context\n",
    "knowledge_context = create_context_for_question(\n",
    "    G=G,\n",
    "    conceptnet_map=conceptnet_map,\n",
    "    question=question,\n",
    "    question_concept=question_concept,\n",
    "    keywords=keywords,\n",
    "    choices=choices,\n",
    "    embedder=embedder,\n",
    "    max_hops=2,\n",
    "    top_k_paths_per_pair=40,\n",
    "    top_k_triples_per_choice=12,\n",
    "    facts_per_choice_limit=6\n",
    ")\n",
    "\n",
    "print(knowledge_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371253c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640943b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27daea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1221"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"data/commonsenseqa_validation.json\"\n",
    "\n",
    "with open(dataset_path, \"r\") as fp:\n",
    "    csqa_dataset = json.load(fp)\n",
    "\n",
    "len(csqa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aee84a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1afa02df02c908a558b4036e80242fac',\n",
       " 'question': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?',\n",
       " 'question_concept': 'revolving door',\n",
       " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
       "  'text': ['bank', 'library', 'department store', 'mall', 'new york']},\n",
       " 'answerKey': 'A'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csqa_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2edd6bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1221/1221 [05:54<00:00,  3.44it/s]\n"
     ]
    }
   ],
   "source": [
    "csqa_knowledge_data = []\n",
    "\n",
    "for item in tqdm(csqa_dataset):\n",
    "    question = item[\"question\"]\n",
    "    question_concept = item[\"question_concept\"]\n",
    "    choices = item[\"choices\"][\"text\"]\n",
    "\n",
    "    conceptnet_map = {\n",
    "        c: conceptnet_relations[c] for c in [question_concept] + choices\n",
    "    }\n",
    "    keywords = extract_keywords_spacy(question)          # extracted keywords\n",
    "\n",
    "    # Build graph\n",
    "    G = build_graph_from_conceptnet_dicts(conceptnet_map, add_reverse_edges=False)\n",
    "\n",
    "    # create prompt/context\n",
    "    knowledge_context = create_context_for_question(\n",
    "        G=G,\n",
    "        conceptnet_map=conceptnet_map,\n",
    "        question=question,\n",
    "        question_concept=question_concept,\n",
    "        keywords=keywords,\n",
    "        choices=choices,\n",
    "        embedder=embedder,\n",
    "        max_hops=2,\n",
    "        top_k_paths_per_pair=40,\n",
    "        top_k_triples_per_choice=12,\n",
    "        facts_per_choice_limit=5\n",
    "    )\n",
    "\n",
    "    csqa_knowledge_data.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"question\": question,\n",
    "        \"concept\": question_concept,\n",
    "        \"choices\": item[\"choices\"],\n",
    "        \"answerKey\": item[\"answerKey\"],\n",
    "        \"knowledge_context\": knowledge_context\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df066c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/csqa_validation_knowledge_context.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(csqa_knowledge_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf68e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
