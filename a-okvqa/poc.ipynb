{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "883b2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0179102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5294a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34661645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55436c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\InstalledApps\\miniconda3\\envs\\my\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21acffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download resources once\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a124356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874b600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e737c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")  # fast, small, good quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from functools import lru_cache\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ‚úÖ ALL 34 ConceptNet Relations\n",
    "RELATION_TEMPLATES = {\n",
    "    \"RelatedTo\": \"{a} is related to {b}.\",\n",
    "    \"FormOf\": \"{a} is an inflected form of {b}.\",\n",
    "    \"IsA\": \"{a} is a type of {b}.\",\n",
    "    \"PartOf\": \"{a} is part of {b}.\",\n",
    "    \"HasA\": \"{a} has a {b}.\",\n",
    "    \"UsedFor\": \"{a} is used for {b}.\",\n",
    "    \"CapableOf\": \"{a} is capable of {b}.\",\n",
    "    \"AtLocation\": \"{a} is typically found at {b}.\",\n",
    "    \"Causes\": \"{a} can cause {b}.\",\n",
    "    \"HasSubevent\": \"{a} includes the event {b}.\",\n",
    "    \"HasFirstSubevent\": \"{a} begins with {b}.\",\n",
    "    \"HasLastSubevent\": \"{a} ends with {b}.\",\n",
    "    \"HasPrerequisite\": \"{a} requires {b} to happen first.\",\n",
    "    \"HasProperty\": \"{a} has the property of being {b}.\",\n",
    "    \"MotivatedByGoal\": \"{a} is done in order to {b}.\",\n",
    "    \"ObstructedBy\": \"{a} can be obstructed by {b}.\",\n",
    "    \"Desires\": \"{a} desires {b}.\",\n",
    "    \"CreatedBy\": \"{a} is created by {b}.\",\n",
    "    \"Synonym\": \"{a} and {b} have similar meanings.\",\n",
    "    \"Antonym\": \"{a} and {b} are opposites.\",\n",
    "    \"DistinctFrom\": \"{a} is distinct from {b}.\",\n",
    "    \"DerivedFrom\": \"{a} is derived from {b}.\",\n",
    "    \"SymbolOf\": \"{a} symbolizes {b}.\",\n",
    "    \"DefinedAs\": \"{a} is defined as {b}.\",\n",
    "    \"MannerOf\": \"{a} is a way of doing {b}.\",\n",
    "    \"LocatedNear\": \"{a} is located near {b}.\",\n",
    "    \"HasContext\": \"{a} is used in the context of {b}.\",\n",
    "    \"SimilarTo\": \"{a} is similar to {b}.\",\n",
    "    \"EtymologicallyRelatedTo\": \"{a} and {b} share a common etymology.\",\n",
    "    \"EtymologicallyDerivedFrom\": \"{a} is etymologically derived from {b}.\",\n",
    "    \"CausesDesire\": \"{a} makes someone want {b}.\",\n",
    "    \"MadeOf\": \"{a} is made of {b}.\",\n",
    "    \"ReceivesAction\": \"{a} can be {b}.\",\n",
    "    \"ExternalURL\": \"For more information about {a}, see {b}.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f2c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RELATION_TEMPLATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=5000)\n",
    "def query_conceptnet_triples(term, max_results=5, lang=\"en\", return_dict=False):\n",
    "    \"\"\"\n",
    "    Query ConceptNet for all relations of a given term.\n",
    "    Returns natural-language statements or structured triples.\n",
    "\n",
    "    Args:\n",
    "        term (str): The concept to query.\n",
    "        max_results (int): Max number of results to return.\n",
    "        lang (str): Language code (default: 'en').\n",
    "        return_dict (bool): If True, returns structured triples instead of text.\n",
    "\n",
    "    Returns:\n",
    "        list: List of sentences or dicts representing ConceptNet triples.\n",
    "    \"\"\"\n",
    "    term = term.strip().lower().replace(\" \", \"_\")\n",
    "    url = f\"http://api.conceptnet.io/c/{lang}/{term}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Error querying ConceptNet for '{term}': {e}\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for edge in data.get(\"edges\", []):\n",
    "        rel = edge.get(\"rel\", {}).get(\"label\", \"\")\n",
    "        if rel not in RELATION_TEMPLATES:\n",
    "            continue\n",
    "\n",
    "        start = edge[\"start\"].get(\"label\", \"\")\n",
    "        end = edge[\"end\"].get(\"label\", \"\")\n",
    "        if not start or not end or start.lower() == end.lower():\n",
    "            continue\n",
    "\n",
    "        # Handle direction (ConceptNet sometimes reverses)\n",
    "        if f\"/c/{lang}/{term}\" == edge[\"end\"].get(\"@id\", \"\"):\n",
    "            start, end = end, start  # flip direction\n",
    "\n",
    "        template = RELATION_TEMPLATES[rel]\n",
    "        sentence = template.format(a=start, b=end)\n",
    "\n",
    "        triple = {\n",
    "            \"subject\": start,\n",
    "            \"relation\": rel,\n",
    "            \"object\": end,\n",
    "            \"sentence\": sentence\n",
    "        }\n",
    "\n",
    "        # results.append(triple if return_dict else sentence)\n",
    "        results.append(triple)\n",
    "\n",
    "        # if len(results) >= max_results:\n",
    "        #     break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Facts about 'cat':\n",
      "  - {'subject': 'a cat', 'relation': 'AtLocation', 'object': 'my lap', 'sentence': 'a cat is typically found at my lap.'}\n",
      "  - {'subject': 'a cat', 'relation': 'AtLocation', 'object': 'a bed', 'sentence': 'a cat is typically found at a bed.'}\n",
      "  - {'subject': 'a cat', 'relation': 'AtLocation', 'object': 'the windowsill', 'sentence': 'a cat is typically found at the windowsill.'}\n",
      "  - {'subject': 'Cat', 'relation': 'CapableOf', 'object': 'hunt mice', 'sentence': 'Cat is capable of hunt mice.'}\n",
      "  - {'subject': 'A cat', 'relation': 'HasA', 'object': 'four legs', 'sentence': 'A cat has a four legs.'}\n",
      "  - {'subject': 'a cat', 'relation': 'CapableOf', 'object': 'drink water', 'sentence': 'a cat is capable of drink water.'}\n",
      "  - {'subject': 'a cat', 'relation': 'CapableOf', 'object': 'catch a mouse', 'sentence': 'a cat is capable of catch a mouse.'}\n",
      "  - {'subject': 'cat', 'relation': 'RelatedTo', 'object': 'feline', 'sentence': 'cat is related to feline.'}\n",
      "  - {'subject': 'cat', 'relation': 'RelatedTo', 'object': 'animal', 'sentence': 'cat is related to animal.'}\n",
      "  - {'subject': 'A cat', 'relation': 'CapableOf', 'object': 'climb up a tree', 'sentence': 'A cat is capable of climb up a tree.'}\n",
      "  - {'subject': 'cat', 'relation': 'RelatedTo', 'object': 'pet', 'sentence': 'cat is related to pet.'}\n",
      "  - {'subject': 'a cat', 'relation': 'Desires', 'object': 'milk to drink', 'sentence': 'a cat desires milk to drink.'}\n",
      "  - {'subject': 'A cat', 'relation': 'CapableOf', 'object': 'corner a mouse', 'sentence': 'A cat is capable of corner a mouse.'}\n",
      "  - {'subject': 'cat', 'relation': 'DistinctFrom', 'object': 'dog', 'sentence': 'cat is distinct from dog.'}\n",
      "  - {'subject': 'cat', 'relation': 'RelatedTo', 'object': 'kitten', 'sentence': 'cat is related to kitten.'}\n",
      "  - {'subject': 'a cat', 'relation': 'AtLocation', 'object': 'a vet', 'sentence': 'a cat is typically found at a vet.'}\n",
      "  - {'subject': 'a cat', 'relation': 'CapableOf', 'object': 'look at a king', 'sentence': 'a cat is capable of look at a king.'}\n",
      "  - {'subject': 'a cat', 'relation': 'AtLocation', 'object': 'a chair', 'sentence': 'a cat is typically found at a chair.'}\n",
      "  - {'subject': 'a cat', 'relation': 'AtLocation', 'object': 'a table', 'sentence': 'a cat is typically found at a table.'}\n",
      "  - {'subject': 'A cat', 'relation': 'HasProperty', 'object': 'a feline', 'sentence': 'A cat has the property of being a feline.'}\n",
      "\n",
      "üîπ Facts about 'sleep':\n",
      "  - {'subject': 'sleep', 'relation': 'RelatedTo', 'object': 'bed', 'sentence': 'sleep is related to bed.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasSubevent', 'object': 'you dream', 'sentence': 'sleep includes the event you dream.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasPrerequisite', 'object': 'close your eyes', 'sentence': 'sleep requires close your eyes to happen first.'}\n",
      "  - {'subject': 'sleep', 'relation': 'RelatedTo', 'object': 'dream', 'sentence': 'sleep is related to dream.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasPrerequisite', 'object': 'dream', 'sentence': 'sleep requires dream to happen first.'}\n",
      "  - {'subject': 'sleep', 'relation': 'RelatedTo', 'object': 'rest', 'sentence': 'sleep is related to rest.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasPrerequisite', 'object': 'get in bed', 'sentence': 'sleep requires get in bed to happen first.'}\n",
      "  - {'subject': 'sleep', 'relation': 'Causes', 'object': 'going to bed', 'sentence': 'sleep can cause going to bed.'}\n",
      "  - {'subject': 'sleep', 'relation': 'MotivatedByGoal', 'object': 'rest', 'sentence': 'sleep is done in order to rest.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasSubevent', 'object': 'resting', 'sentence': 'sleep includes the event resting.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasSubevent', 'object': 'gathering energy for tomorrow', 'sentence': 'sleep includes the event gathering energy for tomorrow.'}\n",
      "  - {'subject': 'sleep', 'relation': 'RelatedTo', 'object': 'rest', 'sentence': 'sleep is related to rest.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasSubevent', 'object': 'dream', 'sentence': 'sleep includes the event dream.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasPrerequisite', 'object': 'go to bed', 'sentence': 'sleep requires go to bed to happen first.'}\n",
      "  - {'subject': 'sleep', 'relation': 'Causes', 'object': 'having a rest', 'sentence': 'sleep can cause having a rest.'}\n",
      "  - {'subject': 'sleep', 'relation': 'CausesDesire', 'object': 'Being tired', 'sentence': 'sleep makes someone want Being tired.'}\n",
      "  - {'subject': 'sleep', 'relation': 'RelatedTo', 'object': 'bed', 'sentence': 'sleep is related to bed.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasPrerequisite', 'object': 'snoring', 'sentence': 'sleep requires snoring to happen first.'}\n",
      "  - {'subject': 'sleep', 'relation': 'HasPrerequisite', 'object': 'snore', 'sentence': 'sleep requires snore to happen first.'}\n",
      "  - {'subject': 'sleep', 'relation': 'RelatedTo', 'object': 'night', 'sentence': 'sleep is related to night.'}\n",
      "\n",
      "üîπ Facts about 'bridge':\n",
      "  - {'subject': 'bridge', 'relation': 'RelatedTo', 'object': 'water', 'sentence': 'bridge is related to water.'}\n",
      "  - {'subject': 'bridge', 'relation': 'RelatedTo', 'object': 'crossing', 'sentence': 'bridge is related to crossing.'}\n",
      "  - {'subject': 'bridge', 'relation': 'RelatedTo', 'object': 'over', 'sentence': 'bridge is related to over.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'AtLocation', 'object': 'river', 'sentence': 'a bridge is typically found at river.'}\n",
      "  - {'subject': 'bridge', 'relation': 'RelatedTo', 'object': 'river', 'sentence': 'bridge is related to river.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'UsedFor', 'object': 'crossing a river', 'sentence': 'a bridge is used for crossing a river.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'AtLocation', 'object': 'trolls', 'sentence': 'a bridge is typically found at trolls.'}\n",
      "  - {'subject': 'bridge', 'relation': 'RelatedTo', 'object': 'road', 'sentence': 'bridge is related to road.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'UsedFor', 'object': 'cross a river', 'sentence': 'a bridge is used for cross a river.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'UsedFor', 'object': 'cross over a body of water', 'sentence': 'a bridge is used for cross over a body of water.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'AtLocation', 'object': 'homeless person', 'sentence': 'a bridge is typically found at homeless person.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'UsedFor', 'object': 'cross a bay', 'sentence': 'a bridge is used for cross a bay.'}\n",
      "  - {'subject': 'bridge', 'relation': 'RelatedTo', 'object': 'over water', 'sentence': 'bridge is related to over water.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'AtLocation', 'object': 'a beam', 'sentence': 'a bridge is typically found at a beam.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'AtLocation', 'object': 'homeless', 'sentence': 'a bridge is typically found at homeless.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'AtLocation', 'object': 'a hobo', 'sentence': 'a bridge is typically found at a hobo.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'AtLocation', 'object': 'a bum', 'sentence': 'a bridge is typically found at a bum.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'AtLocation', 'object': 'a river', 'sentence': 'a bridge is typically found at a river.'}\n",
      "  - {'subject': 'A bridge ', 'relation': 'CreatedBy', 'object': 'man kind', 'sentence': 'A bridge  is created by man kind.'}\n",
      "  - {'subject': 'a bridge', 'relation': 'UsedFor', 'object': 'cross water', 'sentence': 'a bridge is used for cross water.'}\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example Usage\n",
    "terms = [\"cat\", \"sleep\", \"bridge\"]\n",
    "for t in terms:\n",
    "    print(f\"\\nüîπ Facts about '{t}':\")\n",
    "    for fact in query_conceptnet_triples(t, max_results=7):\n",
    "        print(\"  -\", fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "730fa9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def extract_keywords(question: str, max_terms: int = 3) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts key content words (mainly nouns) from the question.\n",
    "    Cleans, lemmatizes, and removes stopwords/punctuation/numbers.\n",
    "    Returns up to `max_terms` unique keywords.\n",
    "    \"\"\"\n",
    "    if not isinstance(question, str):\n",
    "        raise ValueError(f\"Expected string input, got {type(question)}\")\n",
    "\n",
    "    doc = nlp(question)\n",
    "\n",
    "    # Collect candidate tokens: prioritize noun chunks first\n",
    "    candidates = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        root = chunk.root.lemma_.lower().strip()\n",
    "        if root.isalpha() and root not in STOP_WORDS:\n",
    "            candidates.append(root)\n",
    "\n",
    "    # If too few candidates, add additional strong content words (nouns/adjectives)\n",
    "    if len(candidates) < max_terms:\n",
    "        for token in doc:\n",
    "            if (\n",
    "                token.pos_ in {\"NOUN\", \"PROPN\", \"ADJ\"} and\n",
    "                token.is_alpha and\n",
    "                token.lemma_.lower() not in STOP_WORDS\n",
    "            ):\n",
    "                candidates.append(token.lemma_.lower())\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    cleaned = list(dict.fromkeys(candidates))\n",
    "\n",
    "    return cleaned[:max_terms]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37e766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_triples_by_relevance(question: str, triples: list[dict], top_k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Rank ConceptNet triples by cosine similarity between question and triple sentence.\n",
    "    \"\"\"\n",
    "    if not triples:\n",
    "        return []\n",
    "\n",
    "    question_emb = embedder.encode(question, convert_to_tensor=True)\n",
    "    triple_sents = [t[\"sentence\"] for t in triples]\n",
    "    triple_embs = embedder.encode(triple_sents, convert_to_tensor=True)\n",
    "\n",
    "    scores = util.cos_sim(question_emb, triple_embs)[0]\n",
    "    top_indices = torch.topk(scores, min(top_k, len(scores))).indices.tolist()\n",
    "\n",
    "    ranked = [triples[i] for i in top_indices]\n",
    "    for i, t in enumerate(ranked):\n",
    "        t[\"similarity\"] = float(scores[top_indices[i]])\n",
    "    return ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knowledge_context(question: str, top_k: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve and rank ConceptNet triples by relevance to the question.\n",
    "    \"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    terms = extract_keywords(question)\n",
    "    all_triples = []\n",
    "\n",
    "    for term in terms:\n",
    "        all_triples.extend(query_conceptnet_triples(term))\n",
    "\n",
    "    # Deduplicate triples by (subject, relation, object)\n",
    "    seen = set()\n",
    "    unique_triples = []\n",
    "    for t in all_triples:\n",
    "        # print(f\"Retrieved triple: {t}\")\n",
    "        key = (t[\"subject\"].lower(), t[\"relation\"], t[\"object\"].lower())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_triples.append(t)\n",
    "\n",
    "    # Rank by semantic similarity\n",
    "    ranked_triples = rank_triples_by_relevance(question, unique_triples, top_k=top_k)\n",
    "\n",
    "    # Build context string\n",
    "    sentences = \" \".join([t[\"sentence\"] for t in ranked_triples])\n",
    "\n",
    "    return {\n",
    "        \"triples\": ranked_triples,\n",
    "        \"sentences\": sentences\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Why are the wood platforms strapped to the elephants?\n",
      "üîπ Ranked Triples:\n",
      "(elephants - HasA -> trunks)  [score=0.630]\n",
      "(elephants - HasProperty -> very big)  [score=0.612]\n",
      "(elephants - HasProperty -> large)  [score=0.592]\n",
      "\n",
      "üîπ Knowledge Summary:\n",
      "elephants has a trunks. elephants has the property of being very big. elephants has the property of being large.\n"
     ]
    }
   ],
   "source": [
    "question = \"Why are the wood platforms strapped to the elephants?\"\n",
    "context = get_knowledge_context(question, top_k=3)\n",
    "\n",
    "print(\"üîπ Ranked Triples:\")\n",
    "for t in context[\"triples\"]:\n",
    "    print(f\"({t['subject']} - {t['relation']} -> {t['object']})  [score={t['similarity']:.3f}]\")\n",
    "\n",
    "print(\"\\nüîπ Knowledge Summary:\")\n",
    "print(context[\"sentences\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_augmented_prompt(image_path, question, choices):\n",
    "#     knowledge = get_knowledge_context(question)\n",
    "#     prompt = f\"\"\"\n",
    "# You are a visual question answering model.\n",
    "\n",
    "# Image: {image_path}\n",
    "# Question: {question}\n",
    "\n",
    "# Retrieved Knowledge:\n",
    "# {knowledge}\n",
    "\n",
    "# Answer choices:\n",
    "# {', '.join(choices)}\n",
    "\n",
    "# Based on the image and the knowledge, choose the best answer.\n",
    "# \"\"\"\n",
    "#     return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513f92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73956a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 21.5MB 28.9MB/s 0.7s0.7s<0.0s\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load pretrained detector\n",
    "detector = YOLO('yolov8s.pt')  # small but good baseline\n",
    "\n",
    "def detect_objects(image_path, conf_threshold=0.3, max_objects=10):\n",
    "    \"\"\"\n",
    "    Detect objects in the image and return class labels.\n",
    "    \"\"\"\n",
    "    results = detector(image_path)\n",
    "    labels = []\n",
    "    for r in results:\n",
    "        for c in r.boxes.cls:\n",
    "            labels.append(results[0].names[int(c)])\n",
    "    return list(dict.fromkeys(labels))[:max_objects]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab38e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def expand_conceptnet_terms(term, max_neighbors=3):\n",
    "    \"\"\"\n",
    "    Get semantically related terms for a given term from ConceptNet.\n",
    "    Used as a fallback when no relevant terms found.\n",
    "    \"\"\"\n",
    "    neighbors = []\n",
    "    url = f\"http://api.conceptnet.io/query?node=/c/en/{term}&rel=/r/RelatedTo&limit={max_neighbors}\"\n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get(\"edges\", []):\n",
    "            other = edge[\"end\"][\"label\"] if edge[\"start\"][\"label\"].lower() == term.lower() else edge[\"start\"][\"label\"]\n",
    "            neighbors.append(other.lower())\n",
    "    except Exception:\n",
    "        pass\n",
    "    return list(set(neighbors))\n",
    "\n",
    "def get_visual_question_terms(image_path, question, intersection_first=True, expand_if_empty=True):\n",
    "    \"\"\"\n",
    "    Combines visual and question-derived terms, robustly handling all empty cases.\n",
    "    Returns relevant_terms (for retrieval), plus raw question_terms and visual_terms.\n",
    "    \"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    question_terms = extract_keywords(question)\n",
    "    visual_terms = detect_objects(image_path)\n",
    "\n",
    "    # Step 1: intersection (precise)\n",
    "    if intersection_first:\n",
    "        relevant_terms = list(set(question_terms) & set(visual_terms))\n",
    "    else:\n",
    "        relevant_terms = list(set(question_terms) | set(visual_terms))\n",
    "\n",
    "    # Step 2: if intersection empty, fall back to union\n",
    "    if not relevant_terms:\n",
    "        relevant_terms = list(set(question_terms) | set(visual_terms))\n",
    "\n",
    "    # Step 3: if still empty, use ConceptNet expansion of question terms\n",
    "    if expand_if_empty and not relevant_terms and question_terms:\n",
    "        expanded = []\n",
    "        for qt in question_terms:\n",
    "            expanded.extend(expand_conceptnet_terms(qt))\n",
    "        relevant_terms = list(set(question_terms + expanded))\n",
    "\n",
    "    # Step 4: final safety net\n",
    "    if not relevant_terms:\n",
    "        relevant_terms = question_terms if question_terms else [question]\n",
    "\n",
    "    return relevant_terms, question_terms, visual_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53785ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_terms(terms, max_words=3):\n",
    "    \"\"\"\n",
    "    Clean and filter terms before querying ConceptNet.\n",
    "    Includes stopword removal and lemmatization.\n",
    "    \"\"\"\n",
    "    clean_terms = []\n",
    "    \n",
    "    for t in terms:\n",
    "        if not isinstance(t, str): \n",
    "            continue\n",
    "        \n",
    "        # Lowercase + remove punctuation\n",
    "        t = t.strip().lower()\n",
    "        t = re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = t.split()\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [LEMMATIZER.lemmatize(tok) for tok in tokens if tok not in STOPWORDS]\n",
    "        \n",
    "        # Skip if empty or numeric\n",
    "        if not tokens or all(tok.isnumeric() for tok in tokens):\n",
    "            continue\n",
    "        \n",
    "        # Rejoin meaningful multi-word concepts\n",
    "        processed = \" \".join(tokens)\n",
    "        \n",
    "        # Skip phrases that are too long\n",
    "        if len(tokens) > max_words:\n",
    "            continue\n",
    "        \n",
    "        clean_terms.append(processed)\n",
    "    \n",
    "    # Deduplicate while preserving order\n",
    "    return list(dict.fromkeys(clean_terms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e6d9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grounded_knowledge_context(image_path, question, top_k=5):\n",
    "    relevant_terms, q_terms, v_terms = get_visual_question_terms(image_path, question)\n",
    "    \n",
    "    # üîß Sanitize terms before retrieval\n",
    "    # Use improved sanitization\n",
    "    sanitized_terms = sanitize_terms(relevant_terms)\n",
    "\n",
    "    if not sanitized_terms:\n",
    "        sanitized_terms = sanitize_terms(q_terms)\n",
    "    if not sanitized_terms:\n",
    "        sanitized_terms = [question.split()[0]]  # last-resort fallback\n",
    "    \n",
    "    print(f\"Question terms: {q_terms}\")\n",
    "    print(f\"Visual terms: {v_terms}\")\n",
    "    print(f\"Retrieving ConceptNet for: {sanitized_terms}\")\n",
    "    \n",
    "    all_triples = []\n",
    "    for term in sanitized_terms:\n",
    "        triples = query_conceptnet_triples(term, max_results=10)\n",
    "        if triples:\n",
    "            all_triples.extend(triples)\n",
    "\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    unique_triples = []\n",
    "    for t in all_triples:\n",
    "        key = (t['subject'].lower(), t['relation'], t['object'].lower())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_triples.append(t)\n",
    "\n",
    "    # If still no triples, gracefully degrade to linguistic similarity\n",
    "    if not unique_triples:\n",
    "        print(\"‚ö†Ô∏è No ConceptNet triples found, using question terms as pseudo-context.\")\n",
    "        sentences = f\"The question is about {', '.join(sanitized_terms)}.\"\n",
    "        return {\n",
    "            \"triples\": [],\n",
    "            \"sentences\": sentences,\n",
    "            \"visual_terms\": v_terms,\n",
    "            \"question_terms\": q_terms\n",
    "        }\n",
    "\n",
    "    # Rank triples normally\n",
    "    ranked = rank_triples_by_relevance(question, unique_triples, top_k=top_k)\n",
    "    sentences = \" \".join([t[\"sentence\"] for t in ranked])\n",
    "\n",
    "    return {\n",
    "        \"triples\": ranked,\n",
    "        \"sentences\": sentences,\n",
    "        \"visual_terms\": v_terms,\n",
    "        \"question_terms\": q_terms\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "315a18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_grounded_prompt(image_path, question, choices):\n",
    "#     context = get_grounded_knowledge_context(image_path, question, top_k=5)\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a visual question answering model.\n",
    "\n",
    "#     Image: {image_path}\n",
    "#     Question: {question}\n",
    "\n",
    "#     Visual objects detected: {', '.join(context['visual_terms'])}\n",
    "\n",
    "#     Retrieved Knowledge (ConceptNet):\n",
    "#     {'; '.join([f\"({t['subject']} - {t['relation']} -> {t['object']})\" for t in context['triples']])}\n",
    "\n",
    "#     Knowledge Summary:\n",
    "#     {context['sentences']}\n",
    "\n",
    "#     Answer choices:\n",
    "#     {', '.join(choices)}\n",
    "\n",
    "#     Based on the image, the detected objects, and the knowledge, choose the best answer.\n",
    "#     \"\"\"\n",
    "#     return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "aokvqa_dir = os.getenv('AOKVQA_DIR', r\"C:\\workspace\\misc\\5980\\aokvqa\")\n",
    "coco_dir = os.getenv('COCO_DIR', r\"C:\\workspace\\misc\\5980\\coco\")\n",
    "coco_filtered_dir = os.getenv('COCO_FILTERED_DIR', r\"C:\\workspace\\misc\\5980\\coco_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "302f734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_aokvqa import load_aokvqa, get_coco_path\n",
    "val_aokvqa_dataset = load_aokvqa(aokvqa_dir, 'val')  \n",
    "train_aokvqa_dataset = load_aokvqa(aokvqa_dir, 'train')  \n",
    "test_aokvqa_dataset = load_aokvqa(aokvqa_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd498271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 17056\n",
      "Validation dataset size: 1145\n",
      "Test dataset size: 6702\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_aokvqa_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_aokvqa_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_aokvqa_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c874f4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b48af31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grounded_prompt(image_path, question, choices):\n",
    "    context = get_grounded_knowledge_context(image_path, question, top_k=5)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an intelligent visual question answering system that uses both visual perception and external knowledge.\n",
    "\n",
    "    **Task**:\n",
    "    Answer the question about the image as accurately as possible.\n",
    "    Use the detected visual objects for grounding and the provided knowledge for reasoning.\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Image Path**: {image_path}\n",
    "    **Question**: {question}\n",
    "\n",
    "    **Detected Visual Objects**:\n",
    "    {', '.join(context['visual_terms'])}\n",
    "\n",
    "    **Relevant Knowledge (from ConceptNet and related sources)**:\n",
    "    {context['sentences']}\n",
    "\n",
    "    **Answer Choices**:\n",
    "    {', '.join(choices)}\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Instructions**:\n",
    "    1. First, interpret the question carefully ‚Äî identify what kind of information it asks for (object, color, action, relation, etc.).\n",
    "    2. Then, look at the detected visual objects and find which ones are relevant.\n",
    "    3. Use the knowledge summary to fill in facts or relationships that are not directly visible in the image.\n",
    "    4. If the knowledge seems unrelated or missing, rely on the image context alone.\n",
    "    5. Finally, choose **one best answer** from the choices that fits both the visual evidence and the external knowledge.\n",
    "\n",
    "    **Respond only with the final answer choice.**\n",
    "    \"\"\"\n",
    "    return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da5e46bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\misc\\\\5980\\\\coco\\\\val2017\\\\000000461751.jpg'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_id = 461751\n",
    "question = \"What is in the motorcyclist's mouth?\"\n",
    "choices = [\n",
    "    \"toothpick\",\n",
    "    \"food\",\n",
    "    \"popsicle stick\",\n",
    "    \"cigarette\"\n",
    "]\n",
    "image_path = get_coco_path(\"val\", image_id, coco_dir)\n",
    "image_path   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "699f4d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is in the motorcyclist's mouth?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 C:\\workspace\\misc\\5980\\coco\\val2017\\000000461751.jpg: 576x640 4 persons, 1 car, 1 motorcycle, 135.4ms\n",
      "Speed: 4.5ms preprocess, 135.4ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Question terms: ['mouth', 'motorcyclist']\n",
      "Visual terms: ['person', 'car', 'motorcycle']\n",
      "Retrieving ConceptNet for: ['mouth', 'motorcyclist', 'car', 'motorcycle', 'person']\n",
      "You are an intelligent visual question answering system that uses both visual perception and external knowledge.\n",
      "\n",
      "    **Task**:\n",
      "    Answer the question about the image as accurately as possible.\n",
      "    Use the detected visual objects for grounding and the provided knowledge for reasoning.\n",
      "\n",
      "    ---\n",
      "\n",
      "    **Image Path**: C:\\workspace\\misc\\5980\\coco\\val2017\\000000461751.jpg\n",
      "    **Question**: What is in the motorcyclist's mouth?\n",
      "\n",
      "    **Detected Visual Objects**:\n",
      "    person, car, motorcycle\n",
      "\n",
      "    **Relevant Knowledge (from ConceptNet and related sources)**:\n",
      "    motorcyclist is derived from motorcycle. motorcyclist is related to motorradfahrerin. motorcyclist and biker have similar meanings. motorcyclist is related to motociclista. mouth is related to part.\n",
      "\n",
      "    **Answer Choices**:\n",
      "    toothpick, food, popsicle stick, cigarette\n",
      "\n",
      "    ---\n",
      "\n",
      "    **Instructions**:\n",
      "    1. First, interpret the question carefully ‚Äî identify what kind of information it asks for (object, color, action, relation, etc.).\n",
      "    2. Then, look at the detected visual objects and find which ones are relevant.\n",
      "    3. Use the knowledge summary to fill in facts or relationships that are not directly visible in the image.\n",
      "    4. If the knowledge seems unrelated or missing, rely on the image context alone.\n",
      "    5. Finally, choose **one best answer** from the choices that fits both the visual evidence and the external knowledge.\n",
      "\n",
      "    **Respond only with the final answer choice.**\n"
     ]
    }
   ],
   "source": [
    "grounded_prompt = build_grounded_prompt(\n",
    "    image_path=image_path,\n",
    "    question=question,\n",
    "    choices=choices\n",
    ")\n",
    "print(grounded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1988c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
