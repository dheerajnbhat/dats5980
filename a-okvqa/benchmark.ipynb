{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ba7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from transformers.image_utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45cbc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set random seed for reproducibility across Python, NumPy, and PyTorch.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # For deterministic behavior (slower but fully reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49cbe08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "aokvqa_dir = os.getenv('AOKVQA_DIR', r\"C:\\workspace\\misc\\5980\\aokvqa\")\n",
    "coco_filtered_dir = os.getenv('COCO_FILTERED_DIR', r\"C:\\workspace\\misc\\5980\\coco_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb96b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_aokvqa import load_aokvqa, get_coco_path\n",
    "\n",
    "val_aokvqa_dataset = load_aokvqa(aokvqa_dir, 'val')\n",
    "train_aokvqa_dataset = load_aokvqa(aokvqa_dir, 'train')\n",
    "test_aokvqa_dataset = load_aokvqa(aokvqa_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6b1821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 17056\n",
      "Validation dataset size: 1145\n",
      "Test dataset size: 6702\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_aokvqa_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_aokvqa_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_aokvqa_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbf412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f4b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d001b032",
   "metadata": {},
   "source": [
    "single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1681010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 1807.89it/s]\n",
      "c:\\InstalledApps\\miniconda3\\envs\\my\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
    "# model_name = \"HuggingFaceTB/SmolVLM-256M-Base\"\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(data_point, coco_dir, split):\n",
    "    print(data_point['question_id'])\n",
    "    # 22MexNkBPpdZGX6sxbxVBH\n",
    "\n",
    "    # ./datasets/coco/val2017/000000299207.jpg\n",
    "\n",
    "    print(data_point['question'])\n",
    "    print(data_point['choices'])\n",
    "    # What is the man by the bags awaiting?\n",
    "    # ['skateboarder', 'train', 'delivery', 'cab']\n",
    "\n",
    "    correct_choice = data_point['choices'][ data_point['correct_choice_idx'] ]\n",
    "    print(correct_choice)\n",
    "    # Corrrect: cab\n",
    "\n",
    "    print(data_point['rationales'][0])\n",
    "\n",
    "\n",
    "    question = data_point['question']\n",
    "    choices = data_point['choices']\n",
    "\n",
    "    image_path = get_coco_path(split, data_point['image_id'], coco_dir)\n",
    "    # image = load_image(image_path)\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # mcq_question = f\"Question: {question}. These are the choices: {choices}. Answer:\"\n",
    "    # print(\"Question Template:\", mcq_question)\n",
    "\n",
    "    direct_question = f\"Question: {question}. Give one word answer. Answer:\"\n",
    "    # Create input messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": direct_question}\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Prepare inputs\n",
    "    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(DEVICE)\n",
    "\n",
    "    # Generate outputs\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "    generated_texts = processor.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    return data_point, generated_texts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f9d74e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22jbM6gDxdaMaunuzgrsBB\n",
      "What is in the motorcyclist's mouth?\n",
      "['toothpick', 'food', 'popsicle stick', 'cigarette']\n",
      "cigarette\n",
      "He's smoking while riding.\n"
     ]
    }
   ],
   "source": [
    "data_point = val_aokvqa_dataset[0]\n",
    "\n",
    "data_point, pred = get_prediction(data_point, coco_filtered_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d9b8e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"User:\\n\\n\\n\\n\\nQuestion: What is in the motorcyclist's mouth?. Give one word answer. Answer:\\nAssistant: Bottle.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df52f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22jbM6gDxdaMaunuzgrsBB\n",
      "What is in the motorcyclist's mouth?\n",
      "['toothpick', 'food', 'popsicle stick', 'cigarette']\n",
      "cigarette\n",
      "He's smoking while riding.\n",
      "----\n",
      "2Aq5RiEn7eyfWjEbpuYT2o\n",
      "Which number birthday is probably being celebrated?\n",
      "['one', 'ten', 'nine', 'thirty']\n",
      "thirty\n",
      "There is a birthday cake on the table with the number 30 written in icing.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for data_point in val_aokvqa_dataset[:2]:\n",
    "    data_point, pred = get_prediction(data_point, coco_filtered_dir, 'val')\n",
    "    # print(f\"Pred: {pred}\")\n",
    "    print(\"----\")\n",
    "    predictions.append((data_point, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181ba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2079d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e43a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n",
    "# image2 = load_image(\"https://upload.wikimedia.org/wikipedia/commons/4/47/New_york_times_square-terabass.jpg\")\n",
    "\n",
    "# Suppose you have a list of (messages, image) pairs\n",
    "batch_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Can you describe this image?\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    for _ in range(2)\n",
    "    # Add more message dicts as needed\n",
    "]\n",
    "batch_images = [\n",
    "    image,  # Use your loaded images here\n",
    "    image\n",
    "]\n",
    "\n",
    "# Prepare prompts for each message\n",
    "batch_prompts = [processor.apply_chat_template(msg, add_generation_prompt=True) for msg in batch_messages]\n",
    "\n",
    "# Prepare inputs for the batch\n",
    "batch_inputs = processor(\n",
    "    text=batch_prompts,\n",
    "    images=batch_images,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "batch_inputs = batch_inputs.to(DEVICE)\n",
    "\n",
    "batch_generated_ids = model.generate(**batch_inputs, max_new_tokens=500)\n",
    "batch_generated_texts = processor.batch_decode(\n",
    "    batch_generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87904ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4da1a2d1",
   "metadata": {},
   "source": [
    "Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bd58f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "c:\\InstalledApps\\miniconda3\\envs\\my\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Force left padding for decoder-only generation\n",
    "if hasattr(processor, \"tokenizer\"):\n",
    "    processor.tokenizer.padding_side = \"left\"\n",
    "    if processor.tokenizer.pad_token is None:\n",
    "        processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9f115d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_predictions(data_points, coco_dir, split):\n",
    "    \"\"\"\n",
    "    Generate predictions for a batch of data points.\n",
    "\n",
    "    Args:\n",
    "        data_points (list): List of data point dicts.\n",
    "        coco_dir (str): Directory containing COCO images.\n",
    "        split (str): Dataset split ('train', 'val', 'test').\n",
    "\n",
    "    Returns:\n",
    "        List of (data_point, prediction) tuples.\n",
    "    \"\"\"\n",
    "    # Prepare images and questions\n",
    "    images = []\n",
    "    batch_messages = []\n",
    "    for data_point in data_points:\n",
    "        question = data_point['question']\n",
    "        choices = data_point['choices']\n",
    "        image_path = get_coco_path(split, data_point['image_id'], coco_dir)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        images.append(image)\n",
    "        question_template = f\"Question: {question}. These are the choices: {choices}. Answer:\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": question_template}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        batch_messages.append(messages)\n",
    "\n",
    "    # Prepare prompts for each message\n",
    "    batch_prompts = [processor.apply_chat_template(msg, add_generation_prompt=True) for msg in batch_messages]\n",
    "\n",
    "    # Prepare inputs for the batch\n",
    "    batch_inputs = processor(\n",
    "        text=batch_prompts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    batch_inputs = batch_inputs.to(DEVICE)\n",
    "\n",
    "    # Generate outputs\n",
    "    batch_generated_ids = model.generate(**batch_inputs)\n",
    "    batch_generated_texts = processor.batch_decode(\n",
    "        batch_generated_ids,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    return list(zip(data_points, batch_generated_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9ad5cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "predictions = []\n",
    "for i in range(0, len(val_aokvqa_dataset[:10]), batch_size):  # adjust [:20] as needed\n",
    "    batch = val_aokvqa_dataset[i:i+batch_size]\n",
    "    batch_results = get_batch_predictions(batch, coco_filtered_dir, 'val')\n",
    "    for item in batch_results:\n",
    "        print(\"----\")\n",
    "        predictions.append(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23650fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88a58f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'split': 'val',\n",
       "   'image_id': 461751,\n",
       "   'question_id': '22jbM6gDxdaMaunuzgrsBB',\n",
       "   'question': \"What is in the motorcyclist's mouth?\",\n",
       "   'choices': ['toothpick', 'food', 'popsicle stick', 'cigarette'],\n",
       "   'correct_choice_idx': 3,\n",
       "   'direct_answers': ['cigarette',\n",
       "    'cigarette',\n",
       "    'cigarette',\n",
       "    'cigarette',\n",
       "    'cigarette',\n",
       "    'cigarette',\n",
       "    'cigarette',\n",
       "    'cigarette',\n",
       "    'cigarette',\n",
       "    'cigarette'],\n",
       "   'difficult_direct_answer': False,\n",
       "   'rationales': [\"He's smoking while riding.\",\n",
       "    'The motorcyclist has a lit cigarette in his mouth while he rides on the street.',\n",
       "    'The man is smoking.']},\n",
       "  \"User:\\n\\n\\n\\n\\nQuestion: What is in the motorcyclist's mouth?. These are the choices: ['toothpick', 'food', 'popsicle stick', 'cigarette']. Answer:\\nAssistant: Popsicle stick.\"),\n",
       " ({'split': 'val',\n",
       "   'image_id': 377368,\n",
       "   'question_id': '2Aq5RiEn7eyfWjEbpuYT2o',\n",
       "   'question': 'Which number birthday is probably being celebrated?',\n",
       "   'choices': ['one', 'ten', 'nine', 'thirty'],\n",
       "   'correct_choice_idx': 3,\n",
       "   'direct_answers': ['thirty',\n",
       "    '30th',\n",
       "    'thirty',\n",
       "    'thirty',\n",
       "    'thirty',\n",
       "    '30th',\n",
       "    'thirty',\n",
       "    'thirty',\n",
       "    'thirty',\n",
       "    'thirty'],\n",
       "   'difficult_direct_answer': False,\n",
       "   'rationales': ['There is a birthday cake on the table with the number 30 written in icing.',\n",
       "    'The cake says 30.',\n",
       "    'The numerals three and zero are written on the cake, which indicates the person is 30 years of age as of the birthdate.']},\n",
       "  \"User:\\n\\n\\n\\n\\nQuestion: Which number birthday is probably being celebrated?. These are the choices: ['one', 'ten', 'nine', 'thirty']. Answer:\\nAssistant: One.\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75c005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e12429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
